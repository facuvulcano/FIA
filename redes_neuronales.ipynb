{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redes Neuronales\n",
    "\n",
    "red neuronal mas simple posible:\n",
    "\n",
    "un solo input x y una salida y con un peso w. Va a calcular y^ \n",
    "\n",
    "Funciones de activacion (output): \n",
    "\n",
    "- Si el problema es de regresion la funcion de activacion no es nada (linear unit)\n",
    "\n",
    "- Cuando es clasificacion binaria: funcion sigmoidea (1 / (1 + e**-x))\n",
    "\n",
    "- Cuando es clasificacion multiclase: funcion softmax. Me da la probabilidad de las clases. (e ** -x) / (sumatoria de i a K e**-xi). \n",
    "\n",
    "Funciones de activacion (capas densas):\n",
    "\n",
    "- funcion sigmoide (1 / (1 + e**-x)). No esta muy buena porque satura muy rapido, en cuanto x toma un valor mayor a cero enseguida se hace 1, solo tiene sensibilidad alrededor del 0 cuando es (1/2). Si la funcion de activacion satura muy rapido es dificil manejar el gradiente (produce desvanecimiento o explosion del gradiente lo que no me deja entrenar). Por eso, es que el concepto de Deep learning no exisitia hasta fines de la decada del 90 o 2000 porque no habia solucion al desvanecimiento o explosion. \n",
    "\n",
    "- Algo mejor que la sigmoide: tangente hiperbolica. Va entre -1 y 1 asique es mejor ya que tiene un area de sensibildiad mejor pero aun asi sigue saturando. \n",
    "\n",
    "- La que soluciono todo es la RELU: max(0, x)\n",
    "\n",
    "- SILU. x * sigmoidea: (x / (1 + e**-x))\n",
    "\n",
    "cualquier funcion tan compleja como sea se puede aproximar utilizando una red neuronal\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
