{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redes Neuronales\n",
    "\n",
    "red neuronal mas simple posible:\n",
    "\n",
    "un solo input x y una salida y con un peso w. Va a calcular y^ \n",
    "\n",
    "Funciones de activacion (output): \n",
    "\n",
    "- Si el problema es de regresion la funcion de activacion no es nada (linear unit)\n",
    "\n",
    "- Cuando es clasificacion binaria: funcion sigmoidea (1 / (1 + e**-x))\n",
    "\n",
    "- Cuando es clasificacion multiclase: funcion softmax. Me da la probabilidad de las clases. (e ** -x) / (sumatoria de i a K e**-xi). \n",
    "\n",
    "Funciones de activacion (capas densas):\n",
    "\n",
    "- funcion sigmoide (1 / (1 + e**-x)). No esta muy buena porque satura muy rapido, en cuanto x toma un valor mayor a cero enseguida se hace 1, solo tiene sensibilidad alrededor del 0 cuando es (1/2). Si la funcion de activacion satura muy rapido es dificil manejar el gradiente (produce desvanecimiento o explosion del gradiente lo que no me deja entrenar). Por eso, es que el concepto de Deep learning no exisitia hasta fines de la decada del 90 o 2000 porque no habia solucion al desvanecimiento o explosion. \n",
    "\n",
    "- Algo mejor que la sigmoide: tangente hiperbolica. Va entre -1 y 1 asique es mejor ya que tiene un area de sensibildiad mejor pero aun asi sigue saturando. \n",
    "\n",
    "- La que soluciono todo es la RELU: max(0, x)\n",
    "\n",
    "- SILU. x * sigmoidea: (x / (1 + e**-x))\n",
    "\n",
    "cualquier funcion tan compleja como sea se puede aproximar utilizando una red neuronal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean_squared_error\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[39m# Entrenar el modelo\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m model\u001b[39m.\u001b[39mfit(x_train, y_train, epochs\u001b[39m=\u001b[39m\u001b[39m3000\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[39m# Evaluar el modelo\u001b[39;00m\n\u001b[0;32m     25\u001b[0m x_test \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinspace(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mpi, \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mpi, \u001b[39m100\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Facu\\anaconda3\\envs\\myenv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Facu\\anaconda3\\envs\\myenv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:313\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[39mwith\u001b[39;00m epoch_iterator\u001b[39m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m    312\u001b[0m     \u001b[39mfor\u001b[39;00m step, iterator \u001b[39min\u001b[39;00m epoch_iterator\u001b[39m.\u001b[39menumerate_epoch():\n\u001b[1;32m--> 313\u001b[0m         callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m    314\u001b[0m         logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m    315\u001b[0m         logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pythonify_logs(logs)\n",
      "File \u001b[1;32mc:\\Users\\Facu\\anaconda3\\envs\\myenv\\Lib\\site-packages\\keras\\src\\callbacks\\callback_list.py:98\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_begin\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[0;32m     96\u001b[0m         callback\u001b[39m.\u001b[39mon_epoch_end(epoch, logs)\n\u001b[1;32m---> 98\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_batch_begin\u001b[39m(\u001b[39mself\u001b[39m, batch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     99\u001b[0m     logs \u001b[39m=\u001b[39m logs \u001b[39mor\u001b[39;00m {}\n\u001b[0;32m    100\u001b[0m     \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Definir una funci√≥n continua a aproximar\n",
    "def target_function(x):\n",
    "    return np.tan(x)\n",
    "\n",
    "# Generar datos de entrenamiento\n",
    "x_train = np.linspace(-2 * np.pi, 2 * np.pi, 1000)\n",
    "y_train = target_function(x_train)\n",
    "\n",
    "# Construir el modelo\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=1, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(x_train, y_train, epochs=3000, verbose=0)\n",
    "\n",
    "# Evaluar el modelo\n",
    "x_test = np.linspace(-2 * np.pi, 2 * np.pi, 100)\n",
    "y_test = target_function(x_test)\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# Mostrar los resultados\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(x_test, y_test, label='True Function')\n",
    "plt.plot(x_test, y_pred, label='Neural Network Approximation')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
